# 线性代数

## 1 标量

严格来说，仅包含一个数值被称为**标量（scalar）**。

标量由只有一个元素的张量表示。 下面的代码将实例化两个标量，并执行加法、乘法、除法和指数运算。

```python
import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)
print(x + y, x * y, x / y, x ** y)
```

输出结果：

> ```python
> (tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))
> ```



## 2 向量

  向量可以被视为==**标量值组成的列表**==。 这些标量值被称为向量的**元素（element）**或**分量（component）**。 当向量表示数据集中的样本时，它们的值具有一定的现实意义，例如， 如果我们正在研究医院患者可能面临的心脏病发作风险，可能会用一个向量来表示每个患者， 其分量为最近的生命体征、胆固醇水平、每天运动时间等。

在数学表示法中，向量通常记为粗体、小写的符号 （例如，**x**、**y**和**z**)）。人们通过一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。

```python
import torch

x = torch.arange(4)
print(x)
```

输出结果：

> ```python
> tensor([0, 1, 2, 3])
> ```



我们可以使用下标来引用向量的任一元素，例如可以通过$x_i$来引用第$i$个元素。
注意，元素$x_i$是一个标量，所以我们在引用它时不会加粗。
大量文献认为列向量是向量的默认方向。
在数学中，向量$\mathbf{x}$可以写为：
$$
\mathbf{x} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix},\tag{1}
$$
​	

其中$x_1,\ldots,x_n$是向量的元素。在代码中，我们(**通过张量的索引来访问任一元素**)：

```python
import torch

x = torch.arange(4)
print(x[3])
print(type(x[3]))
```

输出结果：

> tensor(3)
> <class 'torch.Tensor'>



## 3 长度、维度和形状

  向量只是一个数字**数组**，就像每个数组都有一个长度一样，每个向量也是如此。在数学表示法中，如果我们想说一个向量$\mathbf{x}$由$n$个实值标量组成，可以将其表示为$\mathbf{x}\in\mathbb{R}^n$。
向量的长度通常称为向量的**维度（dimension）**。

>   **维度（dimension）**这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。为了清楚起见，在此明确一下：
>
> 1. **向量** 或**轴** 的维度被用来表示向量或轴的**长度**，即向量或轴的**元素数量**。
> 2. **张量**的维度用来表示张量具有的**轴数**。在这个意义上，张量的某个轴的维数就是这个轴的长度。



与普通的Python数组一样，我们可通过调用Python的内置`len()`函数来访问张量的长度:

```python
import torch

x = torch.arange(4)
print(len(x))
```

输出结果：

> 4



当用张量表示一个向量（只有一个轴）时，我们也可以通过`.shape`属性访问向量的长度。
**形状（shape）**是一个**元素组**，列出了张量沿每个轴的长度（维数）。对于==**只有一个轴的张量，形状只有一个元素。**==

```python
import torch

x = torch.arange(4)
print(x.shape)
```

输出结果：

> torch.Size([4])	# 列出了张量沿每个轴的长度（维数）, 只有一个轴的张量，形状只有一个元素



## 4 矩阵

矩阵是有用的数据结构：**它们允许我们组织具有不同模式的数据**。
例如，我们矩阵中的行可能对应于不同的房屋**（数据样本）**，而列可能对应于不同的属性。
因此，尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中，将每个数据样本作为矩阵中的行向量更为常见。
后面的章节将讲到这点，这种规定支持常见的深度学习实践操作。例如，沿着张量的最外轴，我们可以访问或遍历小批量的数据样本。



### 4.1 创建和访问

正如向量将标量从零阶推广到一阶，**矩阵（matrix）**将向量从一阶推广到二阶。
矩阵通常用粗体、大写字母来表示（例如，$\mathbf{X}$、$\mathbf{Y}$和$\mathbf{Z}$），在代码中表示为具有两个轴的**张量**。

数学表示法使用$\mathbf{A} \in \mathbb{R}^{m \times n}$ 来表示矩阵$\mathbf{A}$，其由$m$行和$n$列的实值标量组成。
我们可以将任意矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$视为一个表格，其中每个元素$a_{ij}$属于第$i$行第$j$列：
$$
\mathbf{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}.\tag{2}
$$


对于任意$\mathbf{A} \in \mathbb{R}^{m \times n}$，$\mathbf{A}$的形状是（$m$,$n$）或$m \times n$。

当矩阵具有相同数量的行和列时，其形状将变为正方形，它被称为**方阵（square matrix）**。

> 我们可以通过行索引（$i$）和列索引（$j$）来访问矩阵中的标量元素$a_{ij}$，例如$[\mathbf{A}]_{ij}$。
> 如果没有给出矩阵$\mathbf{A}$的标量元素类型，那么我们可以简单地使用矩阵$\mathbf{A}$的小写字母索引下标$a_{ij}$ 来引用$[\mathbf{A}]_{ij}$。
> 可以将逗号插入到单独的索引中，以将复杂些的下标信息表示清楚，例如$a_{2,3j}$和$[\mathbf{A}]_{2i-1,3}$。



当调用函数来实例化张量时，我们可以**通过指定两个分量$m$和$n$来创建一个形状为$m \times n$的矩阵**：

```python
import torch

A = torch.arange(20).reshape(5, 4)
print(A)
```

输出结果：

> ```python
> tensor([[ 0,  1,  2,  3],
>         [ 4,  5,  6,  7],
>         [ 8,  9, 10, 11],
>         [12, 13, 14, 15],
>         [16, 17, 18, 19]])
> ```



### 4.2 矩阵的转置

当我们交换矩阵的行和列时，结果称为矩阵的**转置（transpose）**。通常用$\mathbf{a}^\top$来表示矩阵的转置，如果$\mathbf{B}=\mathbf{A}^\top$，
则对于任意$i$和$j$，都有$b_{ij}=a_{ji}$。
转置的过程如下所示，于是$m\times n$矩阵转换成了$n \times m$矩阵：$(3)\to(4)$
$$
\mathbf{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}.\tag{3}
$$

$$
\mathbf{A}^\top =
\begin{bmatrix}
    a_{11} & a_{21} & \dots  & a_{m1} \\
    a_{12} & a_{22} & \dots  & a_{m2} \\
    \vdots & \vdots & \ddots  & \vdots \\
    a_{1n} & a_{2n} & \dots  & a_{mn}
\end{bmatrix}.\tag{4}
$$



使用代码访问矩阵的转置：

```python
import torch

A = torch.arange(20).reshape(5, 4)
print(A.T)
```

输出结果：

> ```python
> tensor([[ 0,  4,  8, 12, 16],
>         [ 1,  5,  9, 13, 17],
>         [ 2,  6, 10, 14, 18],
>         [ 3,  7, 11, 15, 19]])
> ```



**对称矩阵（symmetric matrix）**是方阵的一种特殊类型，其转置等于自身：$\mathbf{A} = \mathbf{A}^\top$。
这里定义一个对称矩阵$\mathbf{B}$：

```python
import torch

B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
print(B == B.T) # 让其与自身的转置作比较
```



输出结果：

> ```python
> tensor([[True, True, True],
>         [True, True, True],
>         [True, True, True]])
> ```



## 5 张量

==**就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构**==。
  **张量**是描述具有**任意数量轴**的$n$维数组的通用方法。例如，==向量是一阶张量，矩阵是二阶张量==。
张量用特殊字体的大写字母表示（例如，$\mathsf{X}$、$\mathsf{Y}$和$\mathsf{Z}$），它们的索引机制（例如$x_{ijk}$和$[\mathsf{X}]_{1,2i-1,3}$）与矩阵类似。

当我们开始处理图像时，张量将变得更加重要，图像以$n$维数组形式出现，其中3个轴对应于高度、宽度，以及一个**通道（channel）**轴，用于表示颜色通道（红色、绿色和蓝色）。

这里我们构建一个三阶张量：

```python
import torch

X = torch.arange(24).reshape(2, 3, 4)
print(X)
```



输出结果：

> ```python
> tensor([[[ 0,  1,  2,  3],
>          [ 4,  5,  6,  7],
>          [ 8,  9, 10, 11]],
> 
>         [[12, 13, 14, 15],
>          [16, 17, 18, 19],
>          [20, 21, 22, 23]]])
> ```



## 6 张量算法的基本性质

将两个相同形状的矩阵相加：

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
print("A=")
print(A)
print("B=")
print(B)
print("A+B=")
print(A + B)
```



运行结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> B=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> A+B=
> tensor([[ 0.,  2.,  4.,  6.],
>         [ 8., 10., 12., 14.],
>         [16., 18., 20., 22.],
>         [24., 26., 28., 30.],
>         [32., 34., 36., 38.]])



两个矩阵的按元素乘法称为**Hadamard积（Hadamard product）**（数学符号$\odot$）。
```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
print("A=")
print(A)
print("B=")
print(B)
print("AxB=")
print(A * B)
```



运行结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> B=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> AxB=
> tensor([[  0.,   1.,   4.,   9.],
>         [ 16.,  25.,  36.,  49.],
>         [ 64.,  81., 100., 121.],
>         [144., 169., 196., 225.],
>         [256., 289., 324., 361.]])



将张量乘以或加上一个标量不会改变张量的形状，其中张量的==**每个元素**==**都将与标量相加或相乘**:

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print("A=")
print(A)
print("A+3")
print(A + 3)
print("Ax2=")
print(A*2)
```



运行结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> A+3
> tensor([[ 3.,  4.,  5.,  6.],
>         [ 7.,  8.,  9., 10.],
>         [11., 12., 13., 14.],
>         [15., 16., 17., 18.],
>         [19., 20., 21., 22.]])
> Ax2=
> tensor([[ 0.,  2.,  4.,  6.],
>         [ 8., 10., 12., 14.],
>         [16., 18., 20., 22.],
>         [24., 26., 28., 30.],
>         [32., 34., 36., 38.]])



## 7 降维

计算张量内所有元素之和：

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print("A=")
print(A)
print("sum:")
print(A.sum())
```



运行结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> sum:
> tensor(190.)

注：用数学符号来表示矩阵内所有元素求和：比如矩阵$\mathbf{A}$中元素的和，可以记为$\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}$ 。



`axis` 参数用于指定特定轴进行求和。

基本逻辑：比如`shape[5,4]`，那么当`axis`取`1`时，那么就相当于消去第2个轴，此时就相当于求`sum[5]`。这也就是为什么叫做**`降维`**。

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print("A=")
print(A)
A_sum_axis1 = A.sum(axis=0) # 对每列的元素进行求和，相当于求sum[4]
print(A_sum_axis1)
print(A_sum_axis1.shape)
```



输出结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> tensor([40., 45., 50., 55.])
> torch.Size([4])



沿着行和列对矩阵求和：

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print("A=")
print(A)
print(A.sum(axis=[0, 1])) # 结果和A.sum()相同
```



输出结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> tensor(190.)



求平均值：

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print("A=")
print(A)
print(A.mean(), A.sum() / A.numel())
```



输出结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> tensor(9.5000) tensor(9.5000)



同样，计算平均值的函数也可以沿指定轴降低张量的维度:

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print("A=")
print(A)
print(A.mean(axis=1), A.sum(axis=1) / A.shape[1])  # numel()内不能传参数，故这里使用shape获得列
```



输出结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> tensor([ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000]) tensor([ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000])



## 8 非降维求和

有时候需要**计算总和或均值时保持轴数不变**：

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()
print("A=")
print(A)
sum_A = A.sum(axis=1, keepdims=True)
print(sum_A)
sum_B = B.sum(axis=1)
print(sum_B)
```



输出结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> tensor([[ 6.],
>         [22.],
>         [38.],
>         [54.],
>         [70.]])
> tensor([ 6., 22., 38., 54., 70.])

可以发现，调用`keepdims=True`后，可以使得在求和操作后，原来张量的轴数不变。这样可以方便进行以后的一些操作，由于`sum_A`在对每行进行求和后仍保持两个轴，我们可以**通过广播将`A`除以`sum_A`**。：

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print("A=")
print(A)
sum_A = A.sum(axis=1, keepdims=True)
print(A / sum_A) # 此时sum_A会执行广播机制来扩充自己
```



输出结果:

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> tensor([[0.0000, 0.1667, 0.3333, 0.5000],
>         [0.1818, 0.2273, 0.2727, 0.3182],
>         [0.2105, 0.2368, 0.2632, 0.2895],
>         [0.2222, 0.2407, 0.2593, 0.2778],
>         [0.2286, 0.2429, 0.2571, 0.2714]])



如果我们想计算特定轴的累积总和，还有一个方法是调用`cumsum`函数，此函数**不会沿任何轴降低输入张量的维度**：

```py
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print("A=")
print(A)
print(A.cumsum(axis=0))
```



输出结果：

> A=
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  6.,  8., 10.],
>         [12., 15., 18., 21.],
>         [24., 28., 32., 36.],
>         [40., 45., 50., 55.]])

其中，输出结果第一列的`4`相当于`4+0`，`12`相当于`0+4+8`,`24`相当于`0+4+8+12`。简言之就是返回维度dim中输入元素的**累积和**。

官方案例：

```python
>>> a = torch.randn(10)
>>> a
tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,
         0.1850, -1.1571, -0.4243])
>>> torch.cumsum(a, dim=0)
tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,
        -1.8209, -2.9780, -3.4022])
```

可能你会有疑问，为什么输出中是`-1.3175`而不是`-1.3176`?

这个是精度问题了，如果想要保持更高的精度，可以使用`randn(10).astype`来生成张量。



## 9 点积

给定两个向量$\mathbf{x},\mathbf{y}\in\mathbb{R}^d$，它们的**点积（dot product）**$\mathbf{x}^\top\mathbf{y}$ （或$\langle\mathbf{x},\mathbf{y}\rangle$）是相同位置的按元素乘积的和：$\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$。

```python
import torch

x = torch.ones(4, dtype = torch.float32)
y = torch.ones(4, dtype = torch.float32)
print(x, y, torch.dot(x, y))
```



输出结果：

> tensor([1., 1., 1., 1.]) tensor([1., 1., 1., 1.]) tensor(4.)   # tensor(4.)相当于是1x1 +1x1 + 1x1 + 1x1

需要注意的是，`torch.dot`只支持计算两个具有**相同元素数量**的==**一维**==**张量**的点积。



当然，我们也可以通过执行**按元素乘法，然后进行求和**来表示两个向量的点积：

```python
import torch

x, y = torch.ones(4, dtype = torch.float32), torch.ones(4, dtype = torch.float32)
print(torch.sum(x * y))  # 输出：tensor(4.)
```



点积在很多场合都很有用。例如，给定一组由向量$\mathbf{x} \in \mathbb{R}^d$表示的值，和一组由$\mathbf{w} \in \mathbb{R}^d$表示的权重。 $\mathbf{x}$ 中的值根据权重$\mathbf{w}$的加权和，可以表示为点积$\mathbf{x}^\top \mathbf{w}$。
当权重为非负数且和为1（即$\left(\sum_{i=1}^{d}{w_i}=1\right)$）时，点积表示**加权平均（weighted average）**。
将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。



## 10 矩阵-向量积

**矩阵-向量积（matrix-vector product）**计算过程如下：

让我们将矩阵$\mathbf{A}$用它的行向量表示：
$$
\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix},
$$


其中每个$\mathbf{a}^\top_{i} \in \mathbb{R}^n$都是行向量，表示矩阵的第$i$行。

至于为什么要用$\mathbf{a}^\top_{i}$来表示呢？因为我们说向量都是说列向量来的，这里要表示一行，要加转置。而且转置也是为了方便后面的点积运算。



**矩阵向量积$\mathbf{A}\mathbf{x}$是一个长度为$m$的列向量，其第$i$个元素是点积$\mathbf{a}^\top_i \mathbf{x}$**：
$$
\mathbf{A}\mathbf{x}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix}\mathbf{x}
= \begin{bmatrix}
 \mathbf{a}^\top_{1} \mathbf{x}  \\
 \mathbf{a}^\top_{2} \mathbf{x} \\
\vdots\\
 \mathbf{a}^\top_{m} \mathbf{x}\\
\end{bmatrix}.
$$


```python
import torch

A = torch.arange(20, dtype=torch.float64).reshape(5, 4)
x = torch.arange(4, dtype=torch.float64)

print(A.shape, x.shape, torch.mv(A, x))
```



输出结果:

> torch.Size([5, 4]) torch.Size([4]) tensor([ 14.,  38.,  62.,  86., 110.], dtype=torch.float64)

`torch.mv`: 执行矩阵-向量积。



## 11 矩阵-矩阵乘法

**矩阵-矩阵乘法**（matrix-matrix multiplication）

假设有两个矩阵$\mathbf{A} \in \mathbb{R}^{n \times k}$ 和 $\mathbf{B} \in \mathbb{R}^{k \times m}$：
$$
\mathbf{A}=\begin{bmatrix}
 a_{11} & a_{12} & \cdots & a_{1k} \\
 a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \cdots & a_{nk} \\
\end{bmatrix},\quad
\mathbf{B}=\begin{bmatrix}
 b_{11} & b_{12} & \cdots & b_{1m} \\
 b_{21} & b_{22} & \cdots & b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
 b_{k1} & b_{k2} & \cdots & b_{km} \\
\end{bmatrix}.
$$


用行向量$\mathbf{a}^\top_{i} \in \mathbb{R}^k$表示矩阵$\mathbf{A}$的第$i$行，并让列向量$\mathbf{b}_{j} \in \mathbb{R}^k$作为矩阵$\mathbf{B}$的第$j$列。

要生成矩阵积$\mathbf{C} = \mathbf{A}\mathbf{B}$，最简单的方法是考虑$\mathbf{A}$的行向量和$\mathbf{B}$的列向量:
$$
\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix},
\quad \mathbf{B}=\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}.
$$
当我们简单地将每个元素$c_{ij}$计算为点积$\mathbf{a}^\top_i \mathbf{b}_j$:
$$
\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix}
\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
 \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
 \vdots & \vdots & \ddots &\vdots\\
\mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
\end{bmatrix}.
$$

我们可以将矩阵-矩阵乘法$\mathbf{AB}$看作简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n \times m$矩阵。

![](https://img-blog.csdnimg.cn/56764f682ed242e1a754cbd611293f35.png)



在下面的代码中，我们在`A`和`B`上执行矩阵乘法。这里的`A`是一个5行4列的矩阵，`B`是一个4行3列的矩阵。两者相乘后，我们得到了一个**5行3列**的矩阵:

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = torch.ones(4, 3)
print('A=\n', A)
print('B=\n', B)
print(torch.mm(A, B))
```



输出结果：

> A=
>  tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.],
>         [12., 13., 14., 15.],
>         [16., 17., 18., 19.]])
> B=
>  tensor([[1., 1., 1.],
>         [1., 1., 1.],
>         [1., 1., 1.],
>         [1., 1., 1.]])
> tensor([[ 6.,  6.,  6.],
>         [22., 22., 22.],
>         [38., 38., 38.],
>         [54., 54., 54.],
>         [70., 70., 70.]])



## 12 范数

| $L_1$范数     | 向量元素的绝对值之和                                         | $$\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|$$ |
| ------------- | ------------------------------------------------------------ | ----------------------------------------------------- |
| **$L_2$范数** | **欧几里得范数，最常用来计算向量长度。假设$n$维向量$\mathbf{x}$中的元素是$x_1,\ldots,x_n$，其$L_2$*范数*是向量元素平方和的平方根** | $$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}$$      |

其中，在$L_2$范数中常常省略下标$2$，也就是说$\|\mathbf{x}\|$等同于$\|\mathbf{x}\|_2$。在代码中，我们可以按如下方式计算向量的$L_2$范数。

计算向量的$L_2$范数：

```python
import torch

u = torch.tensor([3.0, -4.0])
print(torch.norm(u))  # 输出结果：tensor(5.)
```



计算向量的$L_1$范数：

```python
import torch

u = torch.tensor([3.0, -4.0])
print(torch.abs(u).sum())  # 输出结果：tensor(7.)
```



详情见文章：https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#id10

























