# 数据操作+数据预处理

[TOC]



## 1 使用切片访问矩阵内的元素

### 1.1 访问单个元素

<img src="https://img-blog.csdnimg.cn/64dd0ac76663485a8ab3274fdafa1c62.png" style="zoom: 33%;" />

### 1.2 访问一行元素

<img src="https://img-blog.csdnimg.cn/cf26074554304717ae213ee3694f242e.png" style="zoom:33%;" />

### 1.3 访问子区域

<img src="https://img-blog.csdnimg.cn/22f23c7eaf9949498c2043f541221674.png" style="zoom:33%;" />

==[1:3]==代表从第一行到第二行(**左闭右开的区间**)；==[1::]==代表访问包括第一列之后的所有列；



### 1.4 访问离散的子区域

<img src="https://img-blog.csdnimg.cn/e8503b5722aa46498c78b5048730b6e5.png" style="zoom:33%;" />

==a[::3,::2]==表示在矩阵中选取每隔 3 行和每隔 2 列的元素



## 2 Pytorch基本操作

==补充知识点：标量是0维张量，向量是1维张量，矩阵是2维张量。==

### 2.1 基于张量的基本操作

#### `torch.range()`

使用 `arange` 创建一个行向量 `x`。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。张量中的每个值都称为张量的 *元素*（element）。例如，张量 `x` 中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。

````python
import torch
x = torch.arange(12)
print(x)
````

运行结果：

> tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])



#### `x.shape`

访问张量（沿每个轴的长度）的*形状* 。

```python
import torch
x = torch.arange(12)
print(x.shape)
```

运行结果：

> torch.Size([12])



#### `x.numel()`

即**Number of elements**，获取张量中元素的总数。

```python
import torch
x = torch.arange(12)
print(x.numel())
```

运行结果：

> 12



#### `x.reshape()`

改变一个张量的形状而不改变元素数量和元素值，可以调用`reshape`函数。 例如，可以把张量`x`从形状为（12,）的行向量转换为形状为（3,4）的矩阵。 这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。

```python
import torch
x = torch.arange(12)
print(x)
x = x.reshape(4, 3)
print(x)
```

运行结果：

> tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
> tensor([[ 0,  1,  2],
>      [ 3,  4,  5],
>      [ 6,  7,  8],
>      [ 9, 10, 11]])



在上面的例子中，为了获得一个3行的矩阵，我们手动指定了它有3行和4列。 幸运的是，我们可以通过`-1`来调用**自动计算维度**的功能。 即我们可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。

```python
import torch
x = torch.arange(12)
print(x)
x = x.reshape(-1, 4)
print(x)
x = x.reshape(6, -1)
print(x)
```



运行结果：

> tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
> tensor([[ 0,  1,  2,  3],
>         [ 4,  5,  6,  7],
>         [ 8,  9, 10, 11]])
> tensor([[ 0,  1],
>         [ 2,  3],
>         [ 4,  5],
>         [ 6,  7],
>         [ 8,  9],
>         [10, 11]])



#### `torch.zeros()/torch.ones()`

创建一个形状为`（2,3,4）`的张量，其中所有元素都设置为`0`。`（2,3,4）`表示2个3行4列的二维数组组成的三维数组。

```python
import torch
x = torch.zeros((2,3,4))	//所有元素设置为0，注意中间要用个括号来表示矩阵
print(x)
y = torch.ones((2,3,4))	//所有元素设置为1
print(y)
```

运行结果：

<img src="https://img-blog.csdnimg.cn/5b1173c4d7464e6f9c67fc2cde87724c.png" style="zoom:50%;" />



#### `torch.randn()`

有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为`（3,4）`的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。

```python
import torch
x = torch.randn(3, 4)
print(x)
```

运行结果：

>```
>tensor([[-1.5184,  1.3125, -0.5011, -0.3763],
>        [ 1.7246, -1.1295, -0.0929, -1.5513],
>        [ 1.2910, -0.5104, -0.1649,  0.5109]])
>```



#### **嵌套列表**

通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。

```python
import torch
x = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) //这个张量有3行4列，第一个维度的长度为3，第二个维度的长度为4
print(x)
```

运行结果：

> ```
> tensor([[2, 1, 4, 3],
>         [1, 2, 3, 4],
>         [4, 3, 2, 1]])
> ```



### 2.2 运算符

#### 基本算数运算符

对于任意具有相同形状的张量， 常见的标准算术运算符（`+`、`-`、`*`、`/`和`**`）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。

```python
import torch
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
print(x + y, x - y, x * y, x / y, x ** y)    # **运算符是求幂运算
```

输出结果：

> ```
> (tensor([ 3.,  4.,  6., 10.]),
>  tensor([-1.,  0.,  2.,  6.]),
>  tensor([ 2.,  4.,  8., 16.]),
>  tensor([0.5000, 1.0000, 2.0000, 4.0000]),
>  tensor([ 1.,  4., 16., 64.]))
> ```



#### `torch.exp()`

```python
import torch
x = torch.tensor([1.0, 2, 4, 8])
torch.exp(x)
print(x)
```

输出结果：

> tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])



#### `torch.cat()`

把多个张量*连结*（concatenate）在一起， 把它们**端对端**地叠起来形成一个更大的张量。

```python
import torch
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(X)
print(Y)
```

输出结果：

> ```python
> tensor([[ 0.,  1.,  2.,  3.],
>         [ 4.,  5.,  6.,  7.],
>         [ 8.,  9., 10., 11.]])
> tensor([[2., 1., 4., 3.],
>         [1., 2., 3., 4.],
>         [4., 3., 2., 1.]])
> ```

接下来进行连结：

```python
import torch
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(torch.cat((X, Y), dim=0))	# 维度0，相当于把整个y对齐拼到x的下一行
print(torch.cat((X, Y), dim=1))	# 维度1，相当于把整个y对齐拼到x的下一列
```

输出结果：

> ```python
> (tensor([[ 0.,  1.,  2.,  3.],
>          [ 4.,  5.,  6.,  7.],
>          [ 8.,  9., 10., 11.],
>          [ 2.,  1.,  4.,  3.],
>          [ 1.,  2.,  3.,  4.],
>          [ 4.,  3.,  2.,  1.]]),
>  tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
>          [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
>          [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
> ```



#### 运算符`==`

有时，我们想通过==逻辑运算符==构建二元张量。
以`X == Y`为例：
对于每个位置，如果`X`和`Y`在该位置相等，则新张量中相应项的值为1。
这意味着逻辑语句`X == Y`在该位置处为真，否则该位置为0。

```python
import torch
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
K = X == Y	# 用K接受新张量
print(K)
```

输出结果：

> ```python
> tensor([[False,  True, False,  True],
>         [False, False, False, False],
>         [False, False, False, False]])
> ```

注意：`X`和`Y`大小需相同，不然会报错

#### `sum()`

对张量中的所有元素进行求和，会产生一个单元素张量。

```python
import torch
X = torch.arange(6, dtype=torch.float32).reshape((2,3))
print(X)
K = X.sum()
print(K)
```

输出结果：

> tensor([[0., 1., 2.],
>         [3., 4., 5.]])
> tensor(15.)



### 2.3 广播机制

在**不同形状**的两个张量上执行按元素操作，需要用到 ***广播机制*（broadcasting mechanism）**

 这种机制的工作方式如下：

1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作。

比如：

```python
import torch
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
print("a:")
print(a)
print("b:")
print(b)
print("After broadcasting mechanism:")
print("a + b:")
print(a + b)	# 通过广播机制，将两个张量相加
```

输出结果：

> a:
> tensor([[0],
>         [1],
>         [2]])
> b:
> tensor([[0, 1]])
> After broadcasting mechanism:
> a + b:
> tensor([[0, 1],
>         [1, 2],
>         [2, 3]])

由于`a`和`b`分别是`3×1`和`1×2`矩阵，如果让它们相加，它们的形状不匹配，但我们可以将两个矩阵==**广播**==为一个更大的`3×2`矩阵，具体来说，就是矩阵`a`将复制它的列来扩充为一个更大的`3×2`矩阵， 矩阵`b`将复制它的行扩充为一个更大的`3×2`矩阵，然后便可以执行a和b按元素相加操作。



### 2.4 节省内存

**运行一些操作可能会导致为新结果分配内存**。

在下面的例子中，我们用Python的`id()`函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行`Y = Y + X`后，我们会发现`id(Y)`指向另一个位置。 这是因为Python首先计算`Y + X`，为结果分配新的内存，然后使`Y`指向内存中的这个新位置。

```python
import torch
X = torch.arange(3).reshape((3, 1))
Y = torch.arange(2).reshape((1, 2))
before = id(Y)
print(before)   # 1508201016640
Y = Y + X       
print(id(Y))    # 1508201016480
id(Y) == before # False
```

这种情况是不可取的，原因有两个：

> 1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。==通常情况下，我们希望原地执行这些更新==；
> 2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

所以如何进行**原地更新**，而非进行频繁地分配新的内存呢？

幸运的是，执行原地操作，非常简单。 我们可以使用**切片表示法**将操作的结果分配给先前分配的数组:

**`Y[:] = <expression>`**

为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同， 使用`zeros_like`来分配一个全0的块。

```python
import torch
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
Z = torch.zeros_like(Y)	# 创建一个与输入的tensor大小相同且元素全为0的新tensor
print(Z)
print('after:id(Z):', id(Z))
Z[:] = X + Y	# 切片表示法
print('before:id(Z):', id(Z))
```

输出结果：

> tensor([[0., 0., 0., 0.],
>         [0., 0., 0., 0.],
>         [0., 0., 0., 0.]])
> after:id(Z): 2008206879424
> before:id(Z): 2008206879424

由于前后的内存地址相同，说明我们成功执行了原地操作！

### 2.5 转换为其他Python对象

#### Tensor->numpy

比如，PyTorch 中的 `torch.Tensor` 类型和 NumPy 中的 `numpy.ndarray` 类型，可以相互转换。

具体地，将 PyTorch 定义的张量转换为 NumPy 张量可以通过 `tensor.numpy()` 函数实现。这个函数会返回一个与原始张量共享内存的 NumPy 数组，因此修改 NumPy 数组的值也会反映到原始张量中。

```python
import torch
a = torch.tensor([1, 2, 3])
b = a.numpy()  # 将 PyTorch 张量 a 转换为 NumPy 数组 b
print(b)       # 输出转换后的数组
# 修改 NumPy 数组 b 中的值
b[0] = 666
# 输出修改 NumPy 数组后的原 PyTorch 张量 a
print(a)
```

输出结果：

> [1 2 3]			    # b 为转换后的 NumPy 数组
> tensor([666,   2,   3])	\# a 已经根据 b 的修改发生了变化

可以看出，通过 `a.numpy()` 将 PyTorch 张量 `a` 转换为 NumPy 数组 `b` 后，修改 `b` 中的某个元素的值，原始张量 `a` 也相应地发生了变化。

总之，**torch张量**和**numpy数组**将==共享它们的底层内存==，就地操作更改一个张量也会同时更改另一个张量。

注：同样，将 NumPy 数组转换为 PyTorch 张量可以使用 `torch.from_numpy()` 函数。这个函数将返回一个与原始数组共享内存的 PyTorch 张量，因此修改 PyTorch 张量的值也会反映到原始数组中。

#### 张量->标量

将大小为1的张量转换为**Python标量**，我们可以调用`item`函数或Python的内置函数。

```python
import torch
a = torch.tensor([3.5])
print(a.item(), float(a), int(a))
```

输出结果：

> 3.5 3.5 3

注：当张量 `a` 中只包含一个元素时，使用 `item()` 函数可以轻松获取该元素的值。但是，如果该张量包含多个元素，则会引发异常。因此，从张量中提取值时，请确保该张量中只有一个元素。

## 3 小结

深度学习存储和操作数据的主要接口是张量（𝑛维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。













